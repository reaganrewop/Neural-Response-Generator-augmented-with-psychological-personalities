{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/dbfs/grimlock/sri/sidework/\")\n",
    "\n",
    "root_path = \"/dbfs/grimlock/sri/sidework/\"\n",
    "root_path = \"/Users/sa/CS/stanford/cs229/Project/Neural-Response-Generator-augmented-with-psychological-personalities/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import autoreload\n",
    "import sys\n",
    "sys.path.append(root_path + \"library/\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Data preprocessing custom package\n",
    "import text_preprocessing\n",
    "from text_preprocessing import Preprocessor\n",
    "\n",
    "pt = Preprocessor()\n",
    "\n",
    "# essentials\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pickle as pkl\n",
    "from scipy import sparse\n",
    "from numpy import asarray\n",
    "from numpy import savetxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# To read the files\n",
    "X_train, X_val, y_train, y_val = np.load(open(root_path + \"data/intermediate/MBTI_datasplit_v5_c4_features.pkl\", \"rb\"), allow_pickle=True)\n",
    "data = pickle.load(open(root_path + \"data/intermediate/MBTI_df_v5_c4.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sa/.venv/dl/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:587: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MLPClassifier' object has no attribute '_best_coefs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-9580572fac86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#gb.fit(X_train_vec_red_svd, y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layer_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter_no_change\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Learning rate: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/dl/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mMLP\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \"\"\"\n\u001b[0;32m-> 1027\u001b[0;31m         return self._fit(X, y, incremental=(self.warm_start and\n\u001b[0m\u001b[1;32m   1028\u001b[0m                                             hasattr(self, \"classes_\")))\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/dl/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;31m# Run the Stochastic optimization solver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_STOCHASTIC_SOLVERS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             self._fit_stochastic(X, y, activations, deltas, coef_grads,\n\u001b[0m\u001b[1;32m    371\u001b[0m                                  intercept_grads, layer_units, incremental)\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/dl/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit_stochastic\u001b[0;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units, incremental)\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m             \u001b[0;31m# restore best weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoefs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_best_coefs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercepts_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_best_intercepts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MLPClassifier' object has no attribute '_best_coefs'"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Simple model training and evaluation\n",
    "#learning_rate = [0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 1.0]\n",
    "#for lr in learning_rate:\n",
    "# n_iter_no_change=1\n",
    "#gb = GradientBoostingClassifier(verbose=True, learning_rate=0.1, max_depth=3, subsample=0.7, n_estimators=1000, warm_start=True)\n",
    "#gb.fit(X_train_vec_red_svd, y_train)\n",
    "mlp = MLPClassifier(max_iter=1000, hidden_layer_sizes=(1000,), verbose=True, warm_start=True, early_stopping=True, n_iter_no_change=20)\n",
    "mlp.fit(X_train, y_train)\n",
    "learning_rate = 0.1\n",
    "print(\"Learning rate: \", learning_rate)\n",
    "print(\"Accuracy score (training): {0:.3f}\".format(mlp.score(X_train, y_train)))\n",
    "print(\"Accuracy score (validation): {0:.3f}\".format(mlp.score(X_val, y_val)))\n",
    "print()\n",
    "\n",
    "# 92 - 70 {early stopping with 3}\n",
    "# 96 - 71 {n_estimator as 200}\n",
    "# 99 - 72 {n_estimator as 300}\n",
    "# 100 - 72 {n_estimator as 400}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3798          161.64m\n",
      "         2           1.3744          161.58m\n",
      "         3           1.3698          160.14m\n",
      "         4           1.3658          158.89m\n",
      "         5           1.3622          157.96m\n",
      "         6           1.3592          157.55m\n",
      "         7           1.3564          157.23m\n",
      "         8           1.3538          156.93m\n",
      "         9           1.3515          156.52m\n",
      "        10           1.3493          156.26m\n",
      "        20           1.3332          153.50m\n",
      "        30           1.3226          151.69m\n",
      "        40           1.3140          151.97m\n",
      "        50           1.3067          151.51m\n",
      "        60           1.3000          152.07m\n",
      "        70           1.2943          151.82m\n",
      "        80           1.2890          150.65m\n",
      "        90           1.2843          148.66m\n",
      "       100           1.2799          146.47m\n",
      "       200           1.2457          131.16m\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Simple model training and evaluation\n",
    "#learning_rate = [0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 1.0]\n",
    "#for lr in learning_rate:\n",
    "# n_iter_no_change=1\n",
    "#gb = GradientBoostingClassifier(verbose=True, learning_rate=0.1, max_depth=3, subsample=0.7, n_estimators=1000, warm_start=True)\n",
    "gb = GradientBoostingClassifier(verbose=True, learning_rate=0.1, max_depth=3, subsample=1, n_estimators=1000, warm_start=True, n_iter_no_change=20)\n",
    "gb.fit(X_train, y_train)\n",
    "#mlp = MLPClassifier(max_iter=1000, hidden_layer_sizes=(1000,), verbose=True, warm_start=True)\n",
    "#mlp.fit(X_train, y_train)\n",
    "learning_rate = 0.1\n",
    "print(\"Learning rate: \", learning_rate)\n",
    "print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train, y_train)))\n",
    "print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_val, y_val)))\n",
    "print()\n",
    "\n",
    "# 92 - 70 {early stopping with 3}\n",
    "# 96 - 71 {n_estimator as 200}\n",
    "# 99 - 72 {n_estimator as 300}\n",
    "# 100 - 72 {n_estimator as 400}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
